{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79e3b75a-6476-44d1-9e73-52eea4f71eb9",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "584208c8-353b-416a-ad83-4c9ba11b9940",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/24 16:49:07 WARN Utils: Your hostname, MacBook-Air-de-Mauricio-5.local resolves to a loopback address: 127.0.0.1; using 192.168.100.74 instead (on interface en0)\n",
      "23/09/24 16:49:07 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/09/24 16:49:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "#from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, VectorIndexer, MinMaxScaler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1296e277-4274-43ed-beee-8fad37f705ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4858e62b-aa65-4288-8204-c81f5845be38",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5251497d-c2d3-43e8-b611-1b0ee5502383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|       A/5 21171|   7.25| null|       S|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|        PC 17599|71.2833|  C85|       C|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|female|26.0|    0|    0|STON/O2. 3101282|  7.925| null|       S|\n",
      "|          4|       1|     1|Futrelle, Mrs. Ja...|female|35.0|    1|    0|          113803|   53.1| C123|       S|\n",
      "|          5|       0|     3|Allen, Mr. Willia...|  male|35.0|    0|    0|          373450|   8.05| null|       S|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = spark.read.csv('train.csv', header = True, inferSchema = True)\n",
    "data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ce3a11-e9f8-40bf-81c8-3f00f0cbf599",
   "metadata": {},
   "source": [
    "## Preparing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21daf350-6c5f-45a8-bd42-82ac39188a0b",
   "metadata": {},
   "source": [
    "> ### Splitting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3399aa5-0619-413c-b6b9-dfd10e8933d8",
   "metadata": {},
   "source": [
    "We will use 80% of data as training data, and 20% as testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3fff57cb-a712-4a40-9abe-8f07de7fccda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Rows: 710  Testing Rows: 181\n"
     ]
    }
   ],
   "source": [
    "splits = data.randomSplit([0.8,0.2])\n",
    "train = splits[0]\n",
    "test = splits[1]\n",
    "train_rows = train.count()\n",
    "test_rows = test.count()\n",
    "print(\"Training Rows:\", train_rows, \" Testing Rows:\", test_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fe35ce-3d2f-47fe-b125-13f466099387",
   "metadata": {},
   "source": [
    "> ### Defining our pipeline for five relevant features (Age,Fare,Sex,Pclass,SibSp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3b2f8117-aca2-4ed8-b82b-74c6a19e7eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing libraries\n",
    "from pyspark.ml.feature import Imputer\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.classification import RandomForestClassifier, NaiveBayes\n",
    "\n",
    "# Create an Imputer transformer to fill null values with median\n",
    "imputer = Imputer(inputCols=[\"Age\", \"Fare\"], outputCols=[\"Age_imputed\", \"Fare_imputed\"], strategy=\"median\")\n",
    "\n",
    "# Modify the VectorAssembler to include the imputed columns\n",
    "numVect = VectorAssembler(inputCols=[\"Fare_imputed\", \"Age_imputed\"], outputCol=\"numFeatures\")\n",
    "\n",
    "# Define the pipeline stages\n",
    "strIdx = StringIndexer(inputCol=\"Sex\", outputCol=\"SexIdx\")\n",
    "catVect = VectorAssembler(inputCols=[\"Pclass\", \"SexIdx\", \"SibSp\"], outputCol=\"catFeatures\")\n",
    "catIdx = VectorIndexer(inputCol=catVect.getOutputCol(), outputCol=\"idxCatFeatures\")\n",
    "minMax = MinMaxScaler(inputCol=numVect.getOutputCol(), outputCol=\"normFeatures\")\n",
    "featVect = VectorAssembler(inputCols=[\"idxCatFeatures\", \"normFeatures\"], outputCol=\"features\")\n",
    "\n",
    "#Logistic Regression\n",
    "lr = LogisticRegression(labelCol=\"Survived\", featuresCol=\"features\", maxIter=10, regParam=0.3)\n",
    "nb = NaiveBayes(labelCol = \"Survived\",featuresCol=\"features\",smoothing=1.0)\n",
    "rf = RandomForestClassifier(labelCol = \"Survived\",featuresCol=\"features\",numTrees=10, maxDepth=5, seed=42)\n",
    "# Define the pipeline\n",
    "pipeline_lr = Pipeline(stages=[strIdx, catVect, catIdx, imputer, numVect, minMax, featVect, lr])\n",
    "pipeline_nb = Pipeline(stages=[strIdx, catVect, catIdx, imputer, numVect, minMax, featVect, nb])\n",
    "pipeline_rf = Pipeline(stages=[strIdx, catVect, catIdx, imputer, numVect, minMax, featVect, rf])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60892a9c-0c5d-4cec-849a-2b4bbd5fa900",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "aa2cea1f-c531-4c75-b352-1212f69d47ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipline_model_lr = pipeline_lr.fit(train)\n",
    "pipline_model_nb = pipeline_nb.fit(train)\n",
    "pipline_model_rf = pipeline_rf.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7515e8bd-45a1-413f-9fe4-0dd6599d3fca",
   "metadata": {},
   "source": [
    "### Generate label predictions with logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "878f5c76-1751-45e5-a795-75df1d2df882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked', 'SexIdx', 'catFeatures', 'idxCatFeatures', 'Age_imputed', 'Fare_imputed', 'numFeatures', 'normFeatures', 'features', 'rawPrediction', 'probability', 'prediction']\n",
      "+------------------------------------------------------+----------+--------+\n",
      "|features                                              |prediction|Survived|\n",
      "+------------------------------------------------------+----------+--------+\n",
      "|[0.0,1.0,1.0,0.13913573538264068,0.4722292033174164]  |1.0       |1       |\n",
      "|[2.0,1.0,0.0,0.015468569817999833,0.32143754712239253]|0.0       |1       |\n",
      "|[2.0,0.0,1.0,0.06104473451835265,0.4847951746670017]  |0.0       |0       |\n",
      "|[1.0,0.0,0.0,0.025374310111545468,0.35913546117114853]|0.0       |1       |\n",
      "|[2.0,1.0,1.0,0.03513366015444757,0.38426740387031916] |0.0       |0       |\n",
      "|[0.0,1.0,1.0,0.2859895551532101,0.35913546117114853]  |1.0       |1       |\n",
      "|[0.0,0.0,1.0,0.10149724044618187,0.5224930887157577]  |0.0       |0       |\n",
      "|[2.0,1.0,2.0,0.03513366015444757,0.22090977632570996] |0.0       |0       |\n",
      "|[2.0,1.0,1.0,0.021942337075458514,0.17064589092736868]|0.0       |1       |\n",
      "|[2.0,1.0,0.0,0.01537917417160685,0.23347574767529528] |0.0       |1       |\n",
      "+------------------------------------------------------+----------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction_lr = pipline_model_lr.transform(test)\n",
    "print(prediction_lr.columns)\n",
    "\n",
    "predicted_lr = prediction_lr.select(\"features\",\"prediction\",\"Survived\")\n",
    "predicted_lr.show(10,truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb92611-4516-43d2-bb75-291db727bef1",
   "metadata": {},
   "source": [
    "### Acuraccy, Recall, F1-Score for logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "a4180d15-0ef6-437f-b980-cb95a0b9d06d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of random forest: 0.7569060773480663\n",
      "Recall of random forest: 0.6756756756756757\n",
      "F1 score for logistic regression: 0.7101449275362319\n"
     ]
    }
   ],
   "source": [
    "# Count the number of correct predictions\n",
    "correct_predictions_lr = predicted_lr.filter(predicted_lr.prediction == predicted_lr.Survived).count()\n",
    "\n",
    "# Calculate the total number of predictions\n",
    "total_predictions_lr = predicted_rf.count()\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy_lr = correct_predictions_lr / total_predictions_lr\n",
    "\n",
    "# Count the number of true positives\n",
    "true_positives_lr = predicted_lr.filter((predicted_lr.prediction == 1) & (predicted_lr.Survived == 1)).count()\n",
    "\n",
    "# Count the number of false negatives\n",
    "false_negatives_lr = predicted_lr.filter((predicted_lr.prediction == 0) & (predicted_lr.Survived == 1)).count()\n",
    "\n",
    "# Calculate the recall\n",
    "recall_lr = true_positives_lr / (true_positives_lr + false_negatives_lr)\n",
    "\n",
    "print(\"Accuracy of random forest:\", accuracy_lr)\n",
    "print(\"Recall of random forest:\", recall_rf)\n",
    "f1_score = 2 * tp / (2 * tp + fp + fn)\n",
    "print(\"F1 score for logistic regression:\", f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65926442-6979-424c-9a9b-31bcc8f67e9a",
   "metadata": {},
   "source": [
    "### Generate label predictions with Naive bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7c60c489-803c-4203-a07e-3d9a7e71607f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked', 'SexIdx', 'catFeatures', 'idxCatFeatures', 'Age_imputed', 'Fare_imputed', 'numFeatures', 'normFeatures', 'features', 'rawPrediction', 'probability', 'prediction']\n",
      "+------------------------------------------------------+----------+--------+\n",
      "|features                                              |prediction|Survived|\n",
      "+------------------------------------------------------+----------+--------+\n",
      "|[0.0,1.0,1.0,0.13913573538264068,0.4722292033174164]  |1.0       |1       |\n",
      "|[2.0,1.0,0.0,0.015468569817999833,0.32143754712239253]|1.0       |1       |\n",
      "|[2.0,0.0,1.0,0.06104473451835265,0.4847951746670017]  |0.0       |0       |\n",
      "|[1.0,0.0,0.0,0.025374310111545468,0.35913546117114853]|0.0       |1       |\n",
      "|[2.0,1.0,1.0,0.03513366015444757,0.38426740387031916] |1.0       |0       |\n",
      "|[0.0,1.0,1.0,0.2859895551532101,0.35913546117114853]  |1.0       |1       |\n",
      "|[0.0,0.0,1.0,0.10149724044618187,0.5224930887157577]  |0.0       |0       |\n",
      "|[2.0,1.0,2.0,0.03513366015444757,0.22090977632570996] |1.0       |0       |\n",
      "|[2.0,1.0,1.0,0.021942337075458514,0.17064589092736868]|1.0       |1       |\n",
      "|[2.0,1.0,0.0,0.01537917417160685,0.23347574767529528] |1.0       |1       |\n",
      "+------------------------------------------------------+----------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction_nb = pipline_model_nb.transform(test)\n",
    "print(prediction_nb.columns)\n",
    "\n",
    "predicted_nb = prediction_nb.select(\"features\",\"prediction\",\"Survived\")\n",
    "predicted_nb.show(10,truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb09a8f-68cd-4502-bd21-937fb0ddfdb1",
   "metadata": {},
   "source": [
    "### Acuraccy, Recall and F1-Score for Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "970f15b6-40d1-482d-8376-b66a399df9b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for logistic regression: 0.7790055248618785\n",
      "Recall for logistic regression: 0.6621621621621622\n",
      "F1 score for logistic regression: 0.7101449275362319\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Make predictions on the test data\n",
    "prediction_nb = pipline_model_nb.transform(test)\n",
    "\n",
    "# Create a MulticlassClassificationEvaluator object\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"Survived\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = evaluator.evaluate(prediction_nb)\n",
    "\n",
    "# Create a BinaryClassificationEvaluator object\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"Survived\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
    "\n",
    "# Calculate the F1 score\n",
    "tp = prediction_nb.filter((prediction_nb.Survived == 1) & (prediction_nb.prediction == 1)).count()\n",
    "fp = prediction_nb.filter((prediction_nb.Survived == 0) & (prediction_nb.prediction == 1)).count()\n",
    "fn = prediction_nb.filter((prediction_nb.Survived == 1) & (prediction_nb.prediction == 0)).count()\n",
    "recall = tp / (tp + fn)\n",
    "f1_score = 2 * tp / (2 * tp + fp + fn)\n",
    "\n",
    "print(\"Accuracy for logistic regression:\", accuracy)\n",
    "print(\"Recall for logistic regression:\", recall)\n",
    "print(\"F1 score for logistic regression:\", f1_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02c7b58-db6c-4303-b4e4-cd6910bc8523",
   "metadata": {},
   "source": [
    "### Generate predictions with Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f97c3ea4-188a-4fdc-89df-dc444c890f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked', 'SexIdx', 'catFeatures', 'idxCatFeatures', 'Age_imputed', 'Fare_imputed', 'numFeatures', 'normFeatures', 'features', 'rawPrediction', 'probability', 'prediction']\n",
      "+------------------------------------------------------+----------+--------+\n",
      "|features                                              |prediction|Survived|\n",
      "+------------------------------------------------------+----------+--------+\n",
      "|[0.0,1.0,1.0,0.13913573538264068,0.4722292033174164]  |1.0       |1       |\n",
      "|[2.0,1.0,0.0,0.015468569817999833,0.32143754712239253]|1.0       |1       |\n",
      "|[2.0,0.0,1.0,0.06104473451835265,0.4847951746670017]  |0.0       |0       |\n",
      "|[1.0,0.0,0.0,0.025374310111545468,0.35913546117114853]|0.0       |1       |\n",
      "|[2.0,1.0,1.0,0.03513366015444757,0.38426740387031916] |1.0       |0       |\n",
      "|[0.0,1.0,1.0,0.2859895551532101,0.35913546117114853]  |1.0       |1       |\n",
      "|[0.0,0.0,1.0,0.10149724044618187,0.5224930887157577]  |0.0       |0       |\n",
      "|[2.0,1.0,2.0,0.03513366015444757,0.22090977632570996] |1.0       |0       |\n",
      "|[2.0,1.0,1.0,0.021942337075458514,0.17064589092736868]|1.0       |1       |\n",
      "|[2.0,1.0,0.0,0.01537917417160685,0.23347574767529528] |1.0       |1       |\n",
      "+------------------------------------------------------+----------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction_rf = pipline_model_rf.transform(test)\n",
    "print(prediction_rf.columns)\n",
    "\n",
    "predicted_rf = prediction_rf.select(\"features\",\"prediction\",\"Survived\")\n",
    "predicted_rf.show(10,truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98bec20e-99c0-4a8c-8944-3729c65cb2ce",
   "metadata": {},
   "source": [
    "### Acuraccy, Recall and F1 Score for Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "ffc717ae-0f7b-4f94-bf68-f746565380a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of random forest: 0.8066298342541437\n",
      "Recall of random forest: 0.6756756756756757\n",
      "F1 score for logistic regression: 0.7101449275362319\n"
     ]
    }
   ],
   "source": [
    "# Count the number of correct predictions\n",
    "correct_predictions_rf = predicted_rf.filter(predicted_rf.prediction == predicted_rf.Survived).count()\n",
    "\n",
    "# Calculate the total number of predictions\n",
    "total_predictions_rf = predicted_rf.count()\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy_rf = correct_predictions_rf / total_predictions_rf\n",
    "\n",
    "# Count the number of true positives\n",
    "true_positives_rf = predicted_rf.filter((predicted_rf.prediction == 1) & (predicted_rf.Survived == 1)).count()\n",
    "\n",
    "# Count the number of false negatives\n",
    "false_negatives_rf = predicted_rf.filter((predicted_rf.prediction == 0) & (predicted_rf.Survived == 1)).count()\n",
    "\n",
    "# Calculate the recall\n",
    "recall_rf = true_positives_rf / (true_positives_rf + false_negatives_rf)\n",
    "\n",
    "print(\"Accuracy of random forest:\", accuracy_rf)\n",
    "print(\"Recall of random forest:\", recall_rf)\n",
    "f1_score = 2 * tp / (2 * tp + fp + fn)\n",
    "print(\"F1 score for logistic regression:\", f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1639067-dd8a-4bce-a2a8-aa0fa91b1560",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
