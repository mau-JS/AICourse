{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79e3b75a-6476-44d1-9e73-52eea4f71eb9",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "584208c8-353b-416a-ad83-4c9ba11b9940",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/19 20:12:16 WARN Utils: Your hostname, MacBook-Air-de-Mauricio-5.local resolves to a loopback address: 127.0.0.1; using 10.48.217.10 instead (on interface en0)\n",
      "23/09/19 20:12:16 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/09/19 20:12:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/09/19 20:12:17 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "23/09/19 20:12:34 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "#from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, VectorIndexer, MinMaxScaler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1296e277-4274-43ed-beee-8fad37f705ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4858e62b-aa65-4288-8204-c81f5845be38",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5251497d-c2d3-43e8-b611-1b0ee5502383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|       A/5 21171|   7.25| null|       S|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|        PC 17599|71.2833|  C85|       C|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|female|26.0|    0|    0|STON/O2. 3101282|  7.925| null|       S|\n",
      "|          4|       1|     1|Futrelle, Mrs. Ja...|female|35.0|    1|    0|          113803|   53.1| C123|       S|\n",
      "|          5|       0|     3|Allen, Mr. Willia...|  male|35.0|    0|    0|          373450|   8.05| null|       S|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = spark.read.csv('train.csv', header = True, inferSchema = True)\n",
    "data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ce3a11-e9f8-40bf-81c8-3f00f0cbf599",
   "metadata": {},
   "source": [
    "## Preparing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21daf350-6c5f-45a8-bd42-82ac39188a0b",
   "metadata": {},
   "source": [
    "> ### Splitting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3399aa5-0619-413c-b6b9-dfd10e8933d8",
   "metadata": {},
   "source": [
    "We will use 80% of data as training data, and 20% as testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3fff57cb-a712-4a40-9abe-8f07de7fccda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Rows: 728  Testing Rows: 163\n"
     ]
    }
   ],
   "source": [
    "splits = data.randomSplit([0.8,0.2])\n",
    "train = splits[0]\n",
    "test = splits[1]\n",
    "train_rows = train.count()\n",
    "test_rows = test.count()\n",
    "print(\"Training Rows:\", train_rows, \" Testing Rows:\", test_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fe35ce-3d2f-47fe-b125-13f466099387",
   "metadata": {},
   "source": [
    "> ### Defining our pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2f8117-aca2-4ed8-b82b-74c6a19e7eb2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
